defaults:
- _self_
- actor: fsdp
- ref_actor: ${actor}
- critic: ${actor}

train_data:
  path: null
  prompts_per_rollout: null
  responses_per_prompt: 1

test_data:
  path: null
  prompts_per_rollout: null
  responses_per_prompt: 1

actor:
  max_inference_length_per_device: ${actor.max_length_per_device}
  temperature: ${rollout.train_sampling_params.temperature}
  update_per_rollout: 1
  clip: 0.2
  tis_coef: 0.0
  avg_level: token
  freeze_steps: 0
  offload_model: true

  kl:
    coef: 0.0
    type: null # `reward` or `loss`
    reward_estimator: k1
    loss_estimator: k2
    # `k1`, `k2` or `k3`. See http://joschu.net/blog/kl-approx.html.

  entropy:
    coef: 0.0

rollout:
  model_name: ${actor.model_name}
  dtype: bfloat16
  tp_size: 1
  mem_fraction_static: 0.6
  enable_lora: ${actor.use_lora}
  max_lora_rank: ${actor.lora.r}
  lora_target_modules: ${actor.lora.target_modules}
  responses_per_prompt: ${train_data.responses_per_prompt}
  train_sampling_params:
    temperature: 1.0
    max_new_tokens: null
    stop: null
    no_stop_trim: true
  test_sampling_params:
    temperature: 0.0
    max_new_tokens: ${rollout.train_sampling_params.max_new_tokens}
    stop: ${rollout.train_sampling_params.stop}
    no_stop_trim: ${rollout.train_sampling_params.no_stop_trim}
  max_turns: 1
  env_path: null
  dynamic_filtering: true

ref_actor:
  temperature: ${rollout.train_sampling_params.temperature}

critic:
  max_inference_length_per_device: ${critic.max_length_per_device}
  update_per_rollout: ${actor.update_per_rollout}
  clip: 0.5
  avg_level: ${actor.avg_level}
  offload_model: ${actor.offload_model}

adv:
  estimator: reinforce # `reinforce` or `gae`
  gamma: 1.0
  lamda: 1.0
  responses_per_prompt: ${train_data.responses_per_prompt}
  global_norm: false
  norm_var: false
  
trainer:
  project: null
  experiment_name: null
  load_ckpt_from: null
  n_epochs: 1
  test_freq: null
  save_dir: ckpts/${trainer.experiment_name}
  save_freq: null
  use_wandb: true